# Error Analysis: Clinical Coherence in LLM-Generated Patient Personas

*Eval Cycle 1 — Initial pass | Reviewer: clinical domain expert*

---

## Context

This is the first public error analysis from the Tabula Health Generator evaluation process. It follows the methodology outlined in the [eval framework](./EVAL_FRAMEWORK.md): open coding on a sample of annotated traces, followed by axial coding to build a failure taxonomy, followed by classification of each failure mode as either a *specification failure* (something fixable by improving the prompt) or a *generalization failure* (something the LLM should do correctly but doesn't, making it a candidate for an automated evaluator).

The goal of publishing this is twofold: to demonstrate the evaluation methodology in practice, and to share concrete failure patterns that anyone building LLM-based structured clinical data generators is likely to encounter.

**What was reviewed:** An initial sample of ~10 annotated patient persona traces, reviewed by a clinical domain expert (MD). Each trace was generated from a short natural-language prompt describing a patient scenario and annotated with free-text notes on clinical accuracy.

**Model:** Traces were generated by an earlier model version prior to the current generator upgrade.

**Honest limitation:** This sample is too small to claim theoretical saturation. It's a directionally useful first signal — enough to identify the dominant failure categories and act on the most impactful specification gaps before scaling up annotation.

---

## Methodology: Open Coding → Axial Coding

**Open coding** means reading each trace and writing down raw observations without imposing categories. The goal is to stay in "describe what you see" mode rather than jumping to judgments about what category something belongs to. Here's an excerpt from a real annotation:

> *"Asthma is a plausible diagnosis here — would remove Combivent and replace it with Symbicort 2 puffs BID. Could add Cetirizine 10mg daily, Flonase nasal spray. Would remove these labs and add a CBC with normal values. Also consider adding a Bordetella pertussis PCR swab."*

> *"If medication made the patient feel numb, maybe she wouldn't be on sertraline, though? Clonazepam 0.5mg BID PRN is a reasonable med for the anxiety around this."*

> *"For some reason a lot of these patients are marked as age 35."*

**Axial coding** means going back through the raw notes and grouping similar observations into named failure modes. The names should be specific and descriptive — not "clinical error" but "wrong drug for indication" or "prompt contradiction."

**Spec vs. generalization classification** is the step that determines what to do about each failure:
- **Specification failure**: The prompt/instructions don't say what to do, so the LLM guesses. Fix by improving the generator prompt. Don't build evals for these — just fix them.
- **Generalization failure**: The instructions *do* say what to do, but the LLM fails to apply them correctly. This is the subset worth building automated evaluators for.

---

## Failure Taxonomy

14 failure modes emerged from this initial pass. Ordered by frequency.

---

### FM-6 · Wrong Lab Panel for Condition
**Frequency: ~9/10 traces | Classification: Specification failure (mostly)**

The single most common failure, and the one with the most sub-variants:

| Sub-type | Example |
|----------|---------|
| Irrelevant labs included | Unrelated condition labs added without clinical basis |
| Missing indicated labs | No TSH for fatigue/thyroid-relevant presentations; no B12 for malabsorption |
| Individual component instead of panel | "Hematocrit" instead of "Complete blood count" |
| Procedure listed as lab | Polysomnography (a sleep study) placed in the labs array |
| Lab values don't match clinical picture | Normal hemoglobin range for a patient whose diagnosis implies anemia |

**Root cause:** The generator prompt had a condition→lab mapping table covering diabetes, CKD, heart failure, hypothyroidism, anticoagulant use, and hypertension — but was silent on respiratory, GI, psychiatric, sleep, and gender-affirming care contexts. The LLM improvised, often poorly.

The "values don't match clinical picture" sub-type is a generalization failure: the schema explicitly supports setting abnormal synthetic ranges, and the prompt says "clinically robust personas" — the LLM just defaults to normal values even when the diagnosis implies otherwise.

**Fix:** Extend the condition→lab mapping table; require panel-level test names; add a rule distinguishing labs from procedures; add a rule about setting synthetic ranges to reflect clinical state.

---

### FM-1 · Wrong Drug for Indication
**Frequency: ~5/10 traces | Classification: Specification failure**

The LLM selected a clinically inappropriate medication — one that is either wrong for the diagnosis or typical of a different condition with superficial similarity.

Most striking example: **Combivent (ipratropium-albuterol) prescribed for asthma**. Combivent is a combination bronchodilator typically used for COPD. For asthma, the standard of care is an inhaled corticosteroid controller (e.g., budesonide-formoterol/Symbicort) plus a short-acting beta-agonist for rescue. The drugs look similar on the surface — both are inhaled, both treat respiratory conditions — but they're not interchangeable.

Other examples: omeclamox (an H. pylori eradication regimen) prescribed for a GI patient without H. pylori; wrong migraine medication; inappropriate gabapentin dosing (900mg once daily vs. the clinical standard of 300mg three times daily).

**Root cause:** The prompt said "include matching medications" but provided no condition→medication guidance. The LLM's general medical knowledge is imprecise in ways that matter clinically.

**Fix:** Add first-line medication guidance for the conditions the generator commonly encounters.

---

### FM-2 · Missing Indicated Medication
**Frequency: ~5/10 traces | Classification: Specification failure**

A medication clearly indicated by the clinical scenario was simply absent. An asthma patient had no controller inhaler. An allergic rhinitis patient had no antihistamine or intranasal steroid. A patient tapering off benzos had no sleep aid, which would be standard of care.

This is the mirror image of FM-1: the problem isn't that the wrong drug was chosen, it's that the right drug wasn't chosen at all.

**Root cause:** Same gap — no condition→medication mapping in the prompt.

---

### FM-5 · Prompt Contradiction (Internal Consistency)
**Frequency: ~2/10 traces | Classification: Generalization failure ⭐**

This is the most interesting failure category, and the clearest candidate for an automated evaluator.

The generated persona contradicted a specific factual claim in the input prompt. In one case, the prompt stated that the patient felt "numb" on their medication — a common description of SSRI-induced emotional blunting — yet the persona included sertraline (an SSRI).

The instructions say "create a plausible persona from the user's text." The LLM read the prompt, but failed to reason about the implication: if a medication caused the adverse effect the patient is presenting with, don't prescribe that drug class in the persona.

This is actionable as an automated evaluator: extract factual claims from the prompt (stated intolerance, adverse reaction, prior treatment failure), verify none of those contradict the generated medication list. It's checkable without clinical domain knowledge — just logical consistency between input and output.

---

### FM-10 · Temporal Inconsistency
**Frequency: ~4/10 traces | Classification: Generalization failure ⭐**

Medication start dates that don't match the history implied by the prompt. In one case, a patient described as being "on benzos for 10 years" had a prescription start date 13 years in the past. In another, a patient's long-term medication start date was implausibly far back given their age and the nature of the condition.

The prompt instructs the model to "place encounters/procedures accordingly" when the user implies time windows. Date arithmetic is fully deterministic — if today is 2026 and the prompt says "10 years," the start date should be ~2016. The LLM just got the math wrong.

**Automated evaluator approach:** Extract duration claims from prompts (e.g., "on this medication for X years"), extract medication start dates, verify arithmetic. High precision, low cost — no LLM needed.

---

### FM-12 · Age Clustering / Age-Severity Mismatch
**Frequency: ~5/10 traces | Classification: Generalization failure ⭐**

Multiple patients in the sample defaulted to age 35 regardless of what the prompt implied. An "older man with COPD and frequent flare-ups" came back as 68 — not implausible, but the clinical presentation (frequent exacerbations, limited access to care, worsening over time) more naturally suggests late 70s.

This is mode collapse: the LLM has a default it reaches for when the prompt is ambiguous on age, rather than inferring from clinical context.

**Automated evaluator approach:** Map qualitative age descriptors to expected bands ("older adult" → 65+, "teenager" → 13-18). Flag if the generated age falls outside the band implied by the prompt.

---

### FM-11 · Naming Convention Inconsistency
**Frequency: ~4/10 traces | Classification: Specification failure**

No consistent naming convention was applied. Across the sample: "John Doe," "Patient A," "Child with Asthma," and — appropriately — a realistic synthetic name for the nonbinary patient.

"Child with Asthma" as a patient name is clearly wrong. "John Doe" and "Patient A" are acceptable placeholders but inconsistent and jarring. The prompt said "Never include real PHI (no real names)" but gave no guidance on what to use instead.

**Fix:** Specify realistic synthetic name format with examples.

---

### FM-13 · Encounter Documentation Style
**Frequency: ~3/10 traces | Classification: Specification failure**

The `encounter.reason` field is meant to capture a brief clinical note — a 2-5 word phrase like what would appear as an encounter title in an EHR. Instead, the model wrote narrative descriptions or chief-complaint sentences.

> "Wheezing after a gym class" — reads like a chief complaint
> "flare-up" — should be "flare" (clinical convention)
> A two-sentence narrative summary of the encounter history — not how EHRs document encounter reasons

**Root cause:** The field was typed as `string` with no description. The model filled it with whatever seemed descriptive.

**Fix:** Add format guidance to the field description with concrete examples.

---

### FM-7 · Over-Diagnosis / Unsupported Conditions
**Frequency: ~2/10 traces | Classification: Specification failure**

Conditions added to the persona that weren't supported or implied by the prompt. In the most striking case, a complex GI presentation (ultimately celiac disease) came back with IBS, anxiety disorder, and fibromyalgia added — three additional diagnoses not mentioned in the prompt and arguably stigmatizing without clinical basis.

**Root cause:** The prompt said to prefer "clinically robust personas" and listed a condition count target (3-6). The model interpreted this as license to fill out the list with plausible-sounding comorbidities.

**Fix:** Add an explicit grounding rule: "Only include conditions clearly supported or implied by the prompt."

---

### FM-8 · Missing Symptom-Level Diagnoses
**Frequency: ~2/10 traces | Classification: Generalization failure ⭐**

The LLM correctly identified the underlying diagnosis but omitted the symptom-level codes that would appear alongside it in a real chart.

Example: a teenager with "terrible menstrual cramps and heavy bleeding" — the LLM coded *endometriosis* (the structural diagnosis) but omitted *dysmenorrhea* and *menorrhagia* (the symptom diagnoses). In clinical practice, all three would be coded.

This requires knowing that real charts code both the presenting symptom and the established diagnosis. The generator prompt doesn't convey this convention.

**Automated evaluator approach:** If the prompt explicitly names symptoms (cramps, bleeding, pain, numbness), verify those symptoms appear as coded conditions — either as the symptom itself or as the established diagnosis of that symptom.

---

### FM-9 · Wrong Diagnosis Severity
**Frequency: ~1/10 traces | Classification: Specification failure**

The correct diagnostic category was chosen, but the severity was too high for what the prompt implied.

Example: a nonbinary patient dealing with situational depression from family rejection around gender identity → the persona coded *major depressive disorder*. Given the prompt context (circumstantial, reactive, no description of severity or duration meeting MDD criteria), *persistent depressive disorder* (dysthymia) would have been more appropriate.

Aggravating factor: the generator prompt explicitly listed "Major depressive disorder" as a canonical condition example, likely biasing the model toward the severe form.

**Fix:** Remove biased examples from the prompt; add severity calibration guidance.

---

### FM-14 · Encounter Date Staleness
**Frequency: ~2/10 traces | Classification: Specification failure**

Encounter dates clustered in 2023 across multiple traces, regardless of what year the traces were generated. The prompt said "Dates must be explicit ISO" but gave no default recency guidance.

**Fix:** Add: "Default encounter dates to the past 12 months unless otherwise specified."

---

### FM-3 · Wrong Dose or Frequency
**Frequency: ~3/10 traces | Classification: Specification failure**

Medication dose or frequency was clinically atypical. The most concrete example: gabapentin prescribed as 900mg once daily, when the clinical convention is 300mg three times daily (or 300mg at bedtime for sleep). The total daily dose is the same, but the frequency is wrong.

A complicating factor: the medication schema stores `{ name, start }` — there's no structured `dose` or `frequency` field. Dosing appears in the drug name string (e.g., "gabapentin 900mg daily") with no guidance on format or correct values.

---

### FM-4 · Duplicate / Redundant Medication
**Frequency: ~1/10 traces | Classification: Specification failure**

Two drugs from the same therapeutic class prescribed to the same patient. In the observed case: naproxen alongside Duexis (which is ibuprofen + famotidine) — two NSAIDs simultaneously.

No rule in the prompt prohibited same-class co-prescribing.

---

## Classification Summary

| Count | Classification | Action |
|-------|---------------|--------|
| 10 failure modes | **Specification failure** | Fix the generator prompt. Do this before building evaluators. |
| 5 failure modes | **Generalization failure** | Build binary automated evaluators. These persist even with a correct prompt. |

**Specification failures:** FM-1, FM-2, FM-3, FM-4, FM-6 (mostly), FM-7, FM-9, FM-11, FM-13, FM-14

**Generalization failures (evaluator candidates):** FM-5 (prompt contradiction), FM-6 (lab values vs clinical state), FM-8 (missing symptom diagnoses), FM-10 (date arithmetic), FM-12 (age clustering)

---

## What We Did About It

The 10 specification failures were addressed immediately in the generator prompt before running a second annotation cycle. Key changes:

- **Condition→medication guidance**: First-line medication mapping for the conditions the generator commonly encounters (asthma, COPD, migraine, fibromyalgia, psychiatric conditions, GI conditions)
- **Condition→lab mapping**: Extended to cover respiratory, GI, sleep, psychiatric, anemia, and gender-affirming care scenarios
- **Naming convention**: Realistic synthetic names required; "John Doe" and condition-based names prohibited
- **Age calibration**: Explicit age bands for qualitative descriptors; prohibition on defaulting to 35
- **Encounter format**: 2-5 word clinical phrase required; example encounter reasons provided in schema description
- **Grounding rule**: Conditions must be directly supported by the prompt
- **Same-class medication rule**: No two NSAIDs, SSRIs, or other same-class drugs in the same persona
- **Procedure vs. lab distinction**: Polysomnography, spirometry, imaging → procedures array
- **Date anchoring**: Today's date injected into the system prompt; duration arithmetic guidance added
- **Severity calibration**: Biased examples removed; severity guidance added for depression spectrum

The 5 generalization failures are now being developed as binary automated evaluators — one judge per failure mode, each answering a single yes/no question about a specific problem. These will be validated against the annotated traces as ground truth before being incorporated into the evaluation pipeline.

---

## Takeaways for Others Building Similar Systems

A few observations that may be useful if you're building LLM-based structured data generators:

**1. The spec/gen distinction is the most useful early classification.**
Before you build an evaluator, ask: did I actually tell the LLM what to do? A surprising number of "LLM failures" turn out to be specification gaps. Fix those first. The remaining failures — where the LLM has the instructions but doesn't follow them — are the ones worth automating.

**2. The most frequent failures are often the cheapest to fix.**
Wrong labs (FM-6) appeared in nearly every trace and was almost entirely a specification gap — the prompt just didn't say what labs to include for the relevant conditions. Adding a mapping table reduced that failure rate dramatically without any ML work.

**3. Code-based evaluators beat LLM judges for deterministic failures.**
Date arithmetic (FM-10) and age descriptor matching (FM-12) don't need an LLM to evaluate — they're checkable with simple logic against the prompt. These are cheaper, faster, and more precise than LLM judges. Reserve LLM judges for failures that require genuine language understanding (FM-5 prompt contradiction, FM-8 symptom coverage).

**4. Prompt contradiction is a powerful evaluator signal.**
If the user's prompt contains a specific factual claim ("this medication made them feel numb"), verifying that the output doesn't contradict it is a cheap, high-signal check. It's purely logical — no domain knowledge required — and it catches failures that look plausible on the surface.

**5. Mode collapse is real and observable even in small samples.**
Age defaulting to 35 appeared across multiple traces from different clinical scenarios. If you see the same value appearing suspiciously often in your outputs, it's worth checking whether the model is collapsing to a default. An automated statistical check (flag if a field value appears in more than 20% of outputs) can surface this.

---

## Next Steps

1. Run a second annotation cycle on traces generated with the updated prompt to measure the effect of the specification fixes
2. Build and validate automated evaluators E-1 through E-5 against annotated ground truth
3. Publish alignment metrics (true positive / true negative rates per evaluator) once validated

*Eval framework documentation: [docs/EVAL_FRAMEWORK.md](./EVAL_FRAMEWORK.md)*
